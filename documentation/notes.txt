Text clustering:
1. Text clustering is the application of cluster analysis to text-based documents. It uses machine learning and natural language processing (NLP) to understand and categorize unstructured, textual data.
2. Typically, descriptors (sets of words that describe topic matter) are extracted from the document first. Then they are analyzed for the frequency in which they are found in the document compared to other terms. After which, clusters of descriptors can be identified and then auto-tagged.

Uses of Text clustering:
1. Automatic document organization: organisation of document groups into meaningful groups.
2. 


Problems with indic languages:
1. English is known to be an Internet’s number one language.
2. Accuracy has improved from 11% - 97.50% within the past decade.

Document representation:
1. Document representation is concerned about how textual documents should be represented in various tasks, e.g. text processing, retrieval and knowledge discovery and mining. 
2. Its prevailing approach is the vector space model, i.e. a document di is represented as a vector of term weights , where is the collection of terms that occur at least once in the document collection D.

TF-IDF:
1. TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization.
2. IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as "is", "of", and "that", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following.

Feature Extraction:
1. Feature extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing. 
2. A characteristic of these large data sets is a large number of variables that require a lot of computing resources to process

PCA:
1. PCA is a statistical procedure that uses orthogonal transformation to convert a set of observations of co-related variables into a set of values of linearly uncorrelated variables called principal components.

K-PCA:
1.PCA is a linear method. That is it can only be applied to datasets which are linearly separable. It does an excellent job for datasets, which are linearly separable. But, if we use it to non-linear datasets, we might get a result which may not be the optimal dimensionality reduction. Kernel PCA uses a kernel function to project dataset into a higher dimensional feature space, where it is linearly separable. It is similar to the idea of Support Vector Machines.
2.To understand the utility of kernel PCA, particularly for clustering, observe that, while N points cannot in general be linearly separated in {\displaystyle d<N}{\displaystyle d<N} dimensions, they can almost always be linearly separated in {\displaystyle d\geq N}{\displaystyle d\geq N} dimensions. That is, given N points, {\displaystyle \mathbf {x} _{i}}\mathbf {x} _{i}, if we map them to an N-dimensional space with

{\displaystyle \Phi (\mathbf {x} _{i})}\Phi ({\mathbf  {x}}_{i}) where {\displaystyle \Phi :\mathbb {R} ^{d}\to \mathbb {R} ^{N}}\Phi :{\mathbb  {R}}^{d}\to {\mathbb  {R}}^{N},


